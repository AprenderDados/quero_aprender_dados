## üìò Exemplo 5 ‚Äì SCD Type 2 com Hash, `modified_date` e Tr√™s Etapas

Neste exemplo, vamos implementar um controle de hist√≥rico do tipo **SCD Type 2 (Slowly Changing Dimension)**. Esse padr√£o √© usado quando precisamos manter todas as vers√µes anteriores de um registro ‚Äî preservando o hist√≥rico de mudan√ßas.

### üß† O que √© SCD Type 2? 

O SCD Type 2 cria **um novo registro** a cada altera√ß√£o detectada, mantendo a vers√£o anterior com um campo de controle (`valid_to`, `is_current`, etc.). Isso √© essencial para:

- Auditar mudan√ßas
- Gerar relat√≥rios que respeitam o valor vigente em determinado per√≠odo
- Reconstruir o estado hist√≥rico de uma dimens√£o

### üîç Como detectamos mudan√ßas?

Usamos o `hash` calculado sobre as colunas de neg√≥cio (excluindo colunas t√©cnicas) para identificar se algo mudou. Al√©m disso, usamos `modified_date` para garantir que s√≥ atualizamos com dados mais recentes.

---

### üßÆ Como o Hash √© calculado (em PySpark)?

```python
from pyspark.sql.functions import xxhash64, current_date, lit, coalesce, col, concat_ws, date_add, current_timestamp

# Carregar dados
df_source = spark.table("source_bronze.sales_salesorderdetail_insert_duplicates")

# Simular modifica√ß√£o futura (apenas para exemplo)
df_source = df_source.withColumn("ModifiedDate", date_add(df_source["ModifiedDate"], 50))

# Construir hash baseado em todas as colunas
stringified_columns = [coalesce(col(c).cast("string"), lit("NULL")) for c in df_source.columns]
df_source = df_source.withColumn("hash_value", xxhash64(concat_ws("||", *stringified_columns)))

# Adicionar colunas de controle SCD2
df_source = df_source.withColumn("is_current", lit(True)) \
                     .withColumn("start_date", current_timestamp()) \
                     .withColumn("end_date", lit(None).cast("date"))

# Criar view tempor√°ria para ser usada no SQL
df_source.dropDuplicates(["SalesOrderDetailID"]).createOrReplaceTempView("df_source")
```

---

### üîÅ A estrat√©gia SCD2 ocorre em 3 etapas:

#### üîπ Etapa 1 ‚Äì Inserir Novos Registros

```mermaid
flowchart LR
    subgraph Origem
        A1["product_id = 1<br>price = 10"]
        A2["product_id = 2<br>price = 15"]
    end

    subgraph Destino_antes
        B1["(vazio)"]
    end

    A1 --> B1
    A2 --> B1
```

#### üîπ Etapa 2 ‚Äì Finalizar Registros com Hash diferente e `modified_date` mais recente

```mermaid
flowchart LR
    subgraph Origem_Atualizada
        A2_new["product_id = 2<br>price = 18<br>modified = 2024-01-02"]
    end

    subgraph Destino
        A2_old["product_id = 2<br>price = 15<br>modified = 2024-01-01<br>is_current = true"]
    end

    A2_new -- hash diferente e modified_date > --> A2_old
    A2_old --> Finaliza["is_current = false<br>valid_to = now()"]
```

#### üîπ Etapa 3 ‚Äì Inserir Nova Vers√£o (apenas se modified_date for maior)

```mermaid
flowchart LR
    subgraph Tabela_Final
        V1["product_id = 1<br>price = 10<br>is_current = true"]
        V2_old["product_id = 2<br>price = 15<br>is_current = false"]
        V2_new["product_id = 2<br>price = 18<br>is_current = true"]
    end
```

---

### üßæ Exemplo de Dados ‚Äì Antes e Depois

#### üîç Origem (dados da fonte)

| product_id | price | modified_date     | Observa√ß√£o                                     |
|------------|-------|-------------------|------------------------------------------------|
| 1          | 10    | 2024-01-01        | Novo                                           |
| 2          | 15    | 2024-01-01        | Novo                                           |
| 2          | 18    | 2024-01-02        | Atualiza√ß√£o v√°lida                             |
| 2          | 18    | 2024-01-01        | ‚õîÔ∏è Ignorado ‚Äì mesma vers√£o com data antiga     |
| 2          | 15    | 2023-12-30        | ‚õîÔ∏è Ignorado ‚Äì vers√£o antiga, sem impacto       |

#### ‚úÖ Tabela Final (com hist√≥rico)

| product_id | price | modified_date     | valid_from       | valid_to         | is_current | hash_value |
|------------|-------|-------------------|------------------|------------------|------------|------------|
| 1          | 10    | 2024-01-01        | 2024-01-01       | *(null)*         | ‚úÖ true     | A1X...     |
| 2          | 15    | 2024-01-01        | 2024-01-01       | 2024-01-02       | ‚ùå false    | B7F...     |
| 2          | 18    | 2024-01-02        | 2024-01-02       | *(null)*         | ‚úÖ true     | 9ZK...     |

---

### üíª C√≥digo SQL dividido em 3 partes

#### ‚òëÔ∏è Etapa 1 ‚Äì Inser√ß√£o inicial

```sql
MERGE INTO hive_metastore.target_silver.ex5_sales_salesorderdetail AS target
USING df_source AS source
ON target.SalesOrderID = source.SalesOrderID AND target.SalesOrderDetailID = source.SalesOrderDetailID
WHEN NOT MATCHED THEN
  INSERT *
```

#### üõë Etapa 2 ‚Äì Finalizar vers√µes antigas

```sql
MERGE INTO hive_metastore.target_silver.ex5_sales_salesorderdetail AS target
USING df_source AS source
ON target.SalesOrderID = source.SalesOrderID AND target.SalesOrderDetailID = source.SalesOrderDetailID
WHEN MATCHED AND target.hash_value != source.hash_value AND target.is_current = TRUE AND source.ModifiedDate > target.ModifiedDate THEN
  UPDATE SET target.is_current = FALSE,
             target.end_date = CURRENT_DATE
```

#### üîÅ Etapa 3 ‚Äì Inserir nova vers√£o (usando CTE com filtro)

```sql
WITH filtered_source AS (
  SELECT source.*
  FROM df_source AS source
  LEFT JOIN hive_metastore.target_silver.ex5_sales_salesorderdetail AS target
  ON target.SalesOrderID = source.SalesOrderID AND target.SalesOrderDetailID = source.SalesOrderDetailID
  WHERE source.ModifiedDate > target.ModifiedDate
)
MERGE INTO hive_metastore.target_silver.ex5_sales_salesorderdetail AS target
USING filtered_source AS source
ON target.SalesOrderID = source.SalesOrderID AND target.SalesOrderDetailID = source.SalesOrderDetailID AND target.hash_value = source.hash_value 
WHEN NOT MATCHED THEN
  INSERT *
```

---

### üíª C√≥digo PySpark equivalente (vers√£o alternativa)

```python
from delta.tables import DeltaTable

# Etapa 1 - Atualizar registros encerrando vers√µes antigas
DeltaTable.forName(spark, "target_silver.ex5_sales_salesorderdetail")\
  .alias("target")\
  .merge(
    df_source.alias("source"),
    "target.SalesOrderID = source.SalesOrderID AND "
    "target.SalesOrderDetailID = source.SalesOrderDetailID AND "
    "target.is_current = true AND source.hash_value != target.hash_value AND "
    "source.ModifiedDate > target.ModifiedDate"
  )\
  .whenMatchedUpdate(set={
    "is_current": "false",
    "end_date": "current_date()"
  })\
  .execute()

# Etapa 2 - Inserir novas vers√µes filtrando source por modified_date > target
mais_novos = df_source.alias("source").join(
  spark.table("target_silver.ex5_sales_salesorderdetail").alias("target"),
  on=["SalesOrderID", "SalesOrderDetailID"],
  how="left"
).filter("source.ModifiedDate > target.ModifiedDate OR target.SalesOrderID IS NULL")

DeltaTable.forName(spark, "target_silver.ex5_sales_salesorderdetail")\
  .alias("target")\
  .merge(
    mais_novos.alias("source"),
    "target.SalesOrderID = source.SalesOrderID AND "
    "target.SalesOrderDetailID = source.SalesOrderDetailID AND "
    "target.hash_value = source.hash_value"
  )\
  .whenNotMatchedInsertAll()\
  .execute()
```

---

### ‚úÖ Conclus√£o

Este exemplo demonstrou com clareza como implementar o padr√£o **SCD Type 2 com hash e `modified_date`** em um ambiente Lakehouse com Delta Lake. Dividimos o processo em etapas l√≥gicas:

1. Inser√ß√£o de novos registros
2. Finaliza√ß√£o dos registros existentes com mudan√ßas
3. Inser√ß√£o de novas vers√µes com base na data mais recente

Inclu√≠mos c√≥digo completo em SQL e PySpark, tabelas com antes e depois e diagramas para facilitar o entendimento.

Na pr√≥xima etapa, evoluiremos para o **Exemplo 6**, no qual lidaremos com **cargas hist√≥ricas**, utilizando a fun√ß√£o `for loop` para garantir criar uma vers√£o hist√≥rica sem pular mudan√ßas intermedi√°rias

Vamos l√°!

